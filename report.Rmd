---
title: "Bayesian Data Analysis - Changepoint Detection"
author: "Mine Öğretir & Onur Poyraz"
date: "`r format(Sys.Date())`"
output:
  html_document:
    fig_caption: yes
    highlight: breezedark
    number_sections: yes
    theme: darkly
    toc: yes
    toc_depth: 2
    toc_float:
      smooth_scroll: no
    self_contained: no
  word_document:
    toc: yes
    toc_depth: '2'
  pdf_document:
    keep_tex: yes
    fig_caption: yes
    fig_height: 4
    fig_width: 7
    highlight: tango
    number_sections: yes
    toc: yes
    toc_depth: 2
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Loaded packages

```{r packages, echo=TRUE, results='hide', warning=FALSE, message=FALSE}
library(rstan)
library(BaM)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
```

# Introduction

In this project we have implemented three different changepoint detection algorithms. While two of them are detecting two changepoints in a given data, the third one is working online and can detect as many as changepoints which are possible.

In this report, we are comparing these models and sharing statistical workflow of these algorithms. 

# Description of the Problem

Given a sequential data we might interest in the changes in the data, especially these data change in some way, such as a machinery system could goes to failure or economic trends could be changed. If we asume that data is a result from such a generative process, then we can find the changes in this generative process. 

Changepoint detection models aim to detect the changes in signals or time series data. The models of changepoint detection may detect only one changepoint, or multiple changepoints. In our project we implemented three different models. 

- Gamma-Poisson Multiple Changepoint Detection for two changepoints
- Hierarchical Gamma-Poisson Multiple Changepoint Detection for two changepoints

The third model we implemented is an online changepoint model. 

- Bayesian Online Changepoint Detection[*]

The details of the models are given in the following sections.

[*] Adams, Ryan Prescott, and David JC MacKay. "Bayesian online changepoint detection." arXiv preprint arXiv:0710.3742 (2007).


# Description of the Data
## Coal Mining Dataset

We are using Coil Mining Disasters dataset from BaM library (BaM: Functions and Datasets for Books by Jeff Gill). The data is a vector of British Coal Mining Disasters in length 111.

The data is the number of deaths along a time period at the coil mine disasters.

Source: Lynn, R. and Vanhanen, T. (2001). National IQ and Economic Development. Mankind Quarterly LXI, 415-437.

```{r load data}
data <- list(T = length(coal.mining.disasters),
             D = coal.mining.disasters,
             H = 0.01, 
             mean_D = mean(coal.mining.disasters))
```

```{r plot data}
color = 'darkolivegreen4'
color = 'white'
par(bg=NA, fg = color, col.lab=color, col.axis=color, col.main=color)
plot(1856:(1856+110),coal.mining.disasters, type="l", ylab = 'Number of Deaths',xlab='Year',main='Coal Mining Disasters Dataset')
```

## Synthetic Data

Additionally, in order to show the performance of the Bayesian Online Changepoint Model (BOCM) on multiple changepoints, we are using this synthetic data: 

```{r load artificial data}
synthetic_data <- c(read.csv("synthetic_data.csv", header = FALSE))$V1


data_synthetic <- list(T = length(synthetic_data),
                       D = synthetic_data,
                       H = 0.01,
                       mean_D = mean(synthetic_data))
```


```{r plot artificial data}
color = 'darkolivegreen4'
par(bg=NA, fg = color, col.lab=color, col.axis=color, col.main=color)
plot(synthetic_data, type="l")
```

# Model Details and Stan Codes

## Gamma-Poisson Multiple Changepoint Model

The assumptions of this model are:
- There are two changepoints on the time series data, that is there are three intervals.
- The data comes from a Poisson distribution in each interval. 
- The means of the Poisson distribution generated by underlying Gamma distributions. 

The generative process of the model is:

$$
\begin{aligned} e & \sim \text { Gamma }\left(r_{e},1\right) \\
l & \sim \text { Gamma }\left(r_{l},1\right) \\ 
m & \sim \text { Gamma }\left(r_{m},1\right) \\ 
s_1 & \sim \text { Uniform }(1, T) \\ 
s_2 & \sim \text { Uniform }(1, T) \\ 
\end{aligned}
$$


$$
D_{t} \sim \text { Poisson }( \lambda ) \quad \lambda = \left\{\begin{array}{lll}{e} & \text {if }\: t < s_1  \\ 
{l} & \text {if }\:  s_1 <= t < s_2  \\ 
{m} & \text {else }\: 
\end{array}\right.
$$

The likelihood of the model is:

$$
\begin{aligned} p(D | e, l,m) &= \sum_{s_1=1}^{T} \sum_{s_2=s_1}^{T}  p(s_1, s_2, D | e, l, m) \\ &=\sum_{s=1}^{T}  \sum_{s_2=s_1}^{T} p(s_1) p(s_2) p(D | s_1, s_2, e, l,m) \\ &=\sum_{s_1=1}^{T} \sum_{s_2=s_1}^{T} \text { Uniform }(s_1 | 1, T) \text { Uniform }(s_2 | 1, T)  \prod_{t=1}^{T} \text { Poisson }\left(D_{t} | t<s_1 ?\; e : (t<s_2)\; ? \; l :\; m\right) \end{aligned}
$$


We modified the single Exponential-Poisson changepoint[*,**] model to handle two changepoints. And modified the linear single changepoint algorithm to handle two changepoints. The computation of our code is quadratic, $ O((n^2+3n)/2)$, instead of $O(n^3)$. 

[*] Fonnesbeck, Chris, Anand Patil, David Huard, and John Salvatier. 2013. PyMC User’s Guide.

[**] Stan's User Guide, Change Point Models; https://mc-stan.org/docs/2_21/stan-users-guide/change-point-section.html


The Stan code of Gamma-Poisson Multiple Changepoint Model for two changepoints is as follows:

```{r write stan code}
writeLines(readLines("./StanFiles/multiple_changepoint_gamma.stan"))
```

## Hierarchical Multiple Changepoint Model

In the hierarchical multiple changepoint model, we assume that the shape parameters of the priors for the interval means have a common prior. 

$$
\begin{aligned} 
r_{\{e, l, m\}} & \sim \text { Gamma } (\alpha ,1)\\
e & \sim \text { Gamma }\left(r_{e},1\right) \\
l & \sim \text { Gamma }\left(r_{l},1\right) \\ 
m & \sim \text { Gamma }\left(r_{m},1\right) \\ 
s_1 & \sim \text { Uniform }(1, T) \\ 
s_2 & \sim \text { Uniform }(1, T) \\ 
\end{aligned}
$$



$$
D_{t} \sim \text { Poisson }( \lambda ) \quad \lambda = \left\{\begin{array}{lll}{e} & \text {if }\: t < s_1  \\ 
{l} & \text {if }\:  s_1 <= t < s_2  \\ 
{m} & \text {else }\: 
\end{array}\right.
$$


The Stan code of Hierarchical Gamma-Poisson Multiple Changepoint Model for two changepoints is as follows:

```{r write stan code-hierarchical}
writeLines(readLines("./StanFiles/hierar_multiple_changepoint_gamma.stan"))
```

## Bayesian Online Changepoint Detection


In this section we will discuss about online chnage point algorithm and its Stan application with explaining Ryan P. Adams and David J.C. MacKay’s technical report on Bayesian online changepoint detection.

In contrast to previous model we won't assume any change point in this model. Instead, the sequence of observation can be divided into non-overlapping subsequences. We will interested in the transition of this non-overlapping subsequences.

Bayesian online changepoint detection focues on run-length of the sub-sequences from last changepoint and run length is denoted as $r_t$ and it is updated as follows:

\begin{eqnarray}
P\left(r_{t} | r_{t-1}\right)=\left\{\begin{array}{ll}{H\left(r_{t-1}+1\right)} & {\text { if } r_{t}=0} \\ {1-H\left(r_{t-1}+1\right)} & {\text { if } r_{t}=r_{t-1}+1} \\ {0} & {\text { otherwise }}\end{array}\right.
\end{eqnarray}

In which $H(\tau)$ is hazard function which is dependend on current run length. For the simplicity it could be treated as a constant also and we will show it as $H(\tau) = 1/\lambda$.

### Recursive Run Length Estimation

If we assume that we can calculate the predictive distribution of most recent observation given the current run length then we can calculate the marginal predictive distribution by integrating over the posterior distribution.

\begin{eqnarray}
P\left(x_{t+1} \mid \boldsymbol{x}_{1: t}\right)=\sum_{r_{t}} P\left(x_{t+1} \mid r_{t}, \boldsymbol{x}_{t}^{(r)}\right) P\left(r_{t} \mid \boldsymbol{x}_{1: t}\right)
\end{eqnarray}

where posterior distribution of run length at time is;

\begin{eqnarray}
P\left(r_{t} \mid \boldsymbol{x}_{1: t}\right)=\frac{P\left(r_{t}, \boldsymbol{x}_{1: t}\right)}{P\left(\boldsymbol{x}_{1: t}\right)}
\end{eqnarray}

The joint distribution can be calculated by recursive algorithm which is similar to forward algorithm of the HMMs;

\begin{eqnarray}
P\left(r_{t}, \boldsymbol{x}_{1: t}\right) &=& 
\sum_{r_{t-1}} P\left(r_{t}, r_{t-1}, \boldsymbol{x}_{1: t}\right) \\
&=&\sum_{r_{t-1}} P\left(r_{t}, x_{t} \mid r_{t-1}, \boldsymbol{x}_{1: t-1}\right) P\left(r_{t-1}, \boldsymbol{x}_{1: t-1}\right) \\
&=& \sum_{r_{t-1}} P\left(r_{t} \mid r_{t-1}\right) P\left(x_{t} \mid r_{t-1}, \boldsymbol{x}_{t}^{(r)}\right) P\left(r_{t-1}, \boldsymbol{x}_{1: t-1}\right)
\end{eqnarray}

Ultimately, we want to infer both the run-length posterior distribution $p(r_t \mid \boldsymbol{x}_{1:t})$ and the posterior predictive distribution $p(x_{t+1} \mid \boldsymbol{x}_{1:t})$ so that we can predict the next data point given all the data we have seen so far.

We implemented two versions of this model In the first one, we have implemeted the analytical solution of the BOCM. In the second one, we have implemented the Monte Carlo solution of this model. In both of them observations are coming from again Poisson distribution, while the mean of the Poisson distribution comes from the underlying Gamma distribution. 

The Stan code of the analytical solution of BOCM is as follows:

```{r write stan code-analytic}
writeLines(readLines("./StanFiles/online_analytic.stan"))
```

The Stan code of the Monte Carlo solution of BOCM is as follows:

```{r write stan code-mcmc}
writeLines(readLines("./StanFiles/online_mcmc.stan"))
```


# Prior Choices and Fitting the Models 

## Gamma-Poisson Multiple Changepoint Model

### Prior Choices 

We have Gamma priors for interval means and Uniform prior on changepoints. We are using the mean of the data as the Gamma shape for the interval means priors.

$$
\{r_e, r_l, r_m\} = \text {Gamma} (mean(D),1)
$$
Since we do not want to put any constraints on the changepoints, we are using uniform distribution over the data length.

### Fitting the Model

We fit the model for 5 chains and 5000 iterations with 2000 warm-up phase.

```{r fitting multiple gamma,echo=TRUE, results = 'hide', warning=FALSE, message=FALSE}
num_chains = 5
fit_mgamma <- stan('./StanFiles/multiple_changepoint_gamma.stan',
                 data=data,
                 iter=5000, warmup=2000, chains=num_chains,control = list(adapt_delta=0.9)
)
draws_separate_mgamma <- as.data.frame(fit_mgamma)
```


## Hierarchical Multiple Changepoint Model

### Prior Choices 

We are using the mean of the data as the shape of the Gamma distribution on interval mean prior Gamma distribution's shape. That is we set $\alpha = mean(D)$
We are using 1 as scale on all Gamma distribution. So, using the mean of the data as Gamma shape of the priors seems plausable for both models. 

Since we do not want to put any constarints on the changepoints, we are using uniform distribution over the data length.

### Fitting the Model

We fit the model for 5 chains and 6000 iterations with 2000 warm-up phase.

```{r fitting multiple gamma-hierarchical ,echo=TRUE, results = 'hide', warning=FALSE, message=FALSE}
num_chains = 5
fit_hmgamma <- stan('./StanFiles/hierar_multiple_changepoint_gamma.stan',
                 data=data,
                 iter=6000, warmup=2000, chains=num_chains,control = list(adapt_delta=0.99)
)
draws_separate_hmgamma <- as.data.frame(fit_hmgamma)
```


## Bayesian Online Changepoint Detection

### Prior Choices 

We are using the mean of the data as the shape of the Gamma distribution on interval mean prior Gamma distribution's shape. 
Additionaly we have a hazard prior, which corresponds to the rate of the changepoint. In our experiments we are using $H=0.01$, which means a point is a changepoint with $p=H$. 


### Fitting the Model

In the first part we find the analytical solution and we use fixed parameters. 

We fit the model for 5 chain and 2000 iterations with 200 warm-up phase.


```{r fitting-analytic,echo=TRUE, results = 'hide', warning=FALSE, message=FALSE}
fit_analytic <- stan("StanFiles/online_analytic.stan",
                     data=data, 
                     algorithm = "Fixed_param",
                     iter=2500, warmup=500, chains=5)
```


```{r draws-analytic,echo=TRUE, results = 'hide', warning=FALSE, message=FALSE}
draws_analytic <- extract(fit_analytic, pars = c("meas"))
t_draws_analytic <- t(draws_analytic$meas)
```

In this part, we use MCMC algorithm. 

We fit the model for 5 chain and 2000 iterations with 200 warm-up phase.

```{r fitting-mcmc,echo=TRUE, results = 'hide', warning=FALSE, message=FALSE}
fit_mcmc <- stan("StanFiles/online_mcmc.stan",
                 data=data, 
                 iter=2500, warmup=500, chains=5)
```


```{r draws-mcmc,echo=TRUE, results = 'hide', warning=FALSE, message=FALSE}
draws_mcmc <- extract(fit_mcmc, pars = c("meas"))
t_draws_mcmc <- t(draws_mcmc$meas)
```


# Histograms and Divergences of Parameters

## Gamma-Poisson Multiple Changepoint Model

### Histogram of the Parameters

The histograms of the parameters are as follows:

```{r plot s1 histogram}
color = 'darkgoldenrod4'
samples = draws_separate_mgamma

par(bg=NA, fg = color, col.lab=color, col.axis=color, col.main=color)
hist(samples$s1s, breaks = 100, main = 'Histogram of First Changepoint Samples ', 
     xlab = 's_1',xlim = c(0,data$T))
```
```{r plot s2 histogram}
par(bg=NA, fg = color, col.lab=color, col.axis=color, col.main=color)
hist(samples$s2s, breaks = 100, main = 'Histogram of Second Changepoint Samples ', 
     xlab = 's_2',xlim = c(0,data$T))

```
```{r plot means}
color = 'deepskyblue2'
par(bg=NA, fg = color, col.lab=color, col.axis=color, col.main=color)
hist(samples$e, breaks = 100, main = 'Histogram of the Mean of the First Interval', 
     xlab = 'e',xlim = c(0,6))
hist(samples$l, breaks = 100, main = 'Histogram of the Mean of the Second Interval', 
     xlab = 'l',xlim = c(0,6))
hist(samples$m, breaks = 100, main = 'Histogram of the Mean of the Third Interval', 
     xlab = 'm',xlim = c(0,6))
```

### Convergence Diagnostics

The summary of model fit is as follows: 

```{r summary and draw samples}
print(fit_mgamma, pars = c("e", "l","m", "s1s","s2s"),
      probs = c(0.025, 0.5, 0.975),
      digits_summary = 2, include = TRUE)
```

As we can see that the Rhat values for all parameters and changepoint estimations are 1, that is our model fitted very well.  

The effective sample sizes are looking also good. The ration n_eff / n_transitions is greater than 0.001 for all parameters. 

Here we check the tree depth, E-BFMI and divergences. All are looking good. 

```{r Diagnostics-multiple gamma}
check_treedepth(fit_mgamma)
check_energy(fit_mgamma)
check_divergences(fit_mgamma)
```


As we can see from the following plots of the samples, the plot of the chains of the mean of first interval indicates that it is converged. The other means of the second and third intervals are also converged but there are some divergences. 

When we investigate the plots of the chains for first and second changepoint there are some divergences. However we can also see that the divergence is not random. The divergence on the first changepoint is less that the divergence of the second changepoint. As the histograms of the changepoints also shows that there is a slight probability that the first changepoint is closer to the beginning of the data. And also there is some probability that the second changepoint is around 40. This posterior distribution of the changepoints causes the divergence of the chains.    


```{r plot divergences,fig.height=10, fig.width=20}
matplot(draws_separate_mgamma$s1s, type = c("l"),col = 1:num_chains, 
        xlab = 'iteration', ylab = 's1')
matplot(draws_separate_mgamma$s2s, type = c("l"),col = 1:num_chains, 
        xlab = 'iteration', ylab = 's2')
matplot(draws_separate_mgamma$e, type = c("l"),col = 1:num_chains, 
        xlab = 'iteration', ylab = 'e')
matplot(draws_separate_mgamma$l, type = c("l"),col = 1:num_chains, 
        xlab = 'iteration', ylab = 'l')
matplot(draws_separate_mgamma$m, type = c("l"),col = 1:num_chains, 
        xlab = 'iteration', ylab = 'm')
```


## Hierarchical Multiple Changepoint Model

### Histogram of Parameters

The histograms of the parameters are as follows:


```{r plot s1 histogram-hierarchical}
color = 'goldenrod4'
samples = draws_separate_hmgamma

par(bg=NA, fg = color, col.lab=color, col.axis=color, col.main=color)
hist(samples$s1s, breaks = 100, main = 'Histogram of First Changepoint Samples ', 
     xlab = 's_1',xlim = c(0,data$T))
```
```{r plot s2 histogram-hierarchical}
par(bg=NA, fg = color, col.lab=color, col.axis=color, col.main=color)
hist(samples$s2s, breaks = 100, main = 'Histogram of Second Changepoint Samples ',
     xlab = 's_2',xlim = c(0,data$T))

```
```{r plot means-hierarchical}
color = 'deepskyblue4'
par(bg=NA, fg = color, col.lab=color, col.axis=color, col.main=color)
hist(samples$e, breaks = 100, main = 'Histogram of the Mean of the First Interval', 
     xlab = 'e',xlim = c(0,6))
hist(samples$l, breaks = 100, main = 'Histogram of the Mean of the Second Interval', 
     xlab = 'l',xlim = c(0,6))
hist(samples$m, breaks = 100, main = 'Histogram of the Mean of the Third Interval', 
     xlab = 'm',xlim = c(0,6))
```

### Convergence Diagnostics

The summary of the model fit is as follows: 

```{r summary and draw samples-hierarchical}
print(fit_hmgamma, pars = c("e", "l","m", "s1s","s2s"),
      probs = c(0.025, 0.5, 0.975),
      digits_summary = 2, include = TRUE)
```

As we can see that the Rhat values for all parameters and changepoint estimations are 1, that is our model fitted very well.  

The effective sample sizes are looking also good. The ration n_eff / n_transitions is greater than 0.001 for all parameters. 

Here we check the tree depth, E-BFMI and divergences. All are looking good. 

```{r Diagnostics-hierarchical}
check_treedepth(fit_hmgamma)
check_energy(fit_hmgamma)
check_divergences(fit_hmgamma)
```


The same case as in the Non-hierarchical model happens here. The best converged parameter is the mean of the first interval. The divergence on the second changepoint is the highest because of the two possible changepoint locations. 


```{r plot divergences-hierarchical,fig.height=10, fig.width=20}

matplot(draws_separate_mgamma$s1s, type = c("l"),col = 1:num_chains, 
        xlab = 'iter', ylab = 's1')
matplot(draws_separate_mgamma$s2s, type = c("l"),col = 1:num_chains, 
        xlab = 'iter', ylab = 's2')
matplot(draws_separate_mgamma$e, type = c("l"),col = 1:num_chains, 
        xlab = 'iter', ylab = 'e')
matplot(draws_separate_mgamma$l, type = c("l"),col = 1:num_chains, 
        xlab = 'iter', ylab = 'l')
matplot(draws_separate_mgamma$m, type = c("l"),col = 1:num_chains, 
        xlab = 'iter', ylab = 'm')
```

## Bayesian Online Changepoint Detection

### Posterior Heatmaps

#### Analytical Solution

```{r plot online run length, fig.width=15,fig.height=5}
t_draws <- t_draws_analytic
r = matrix(0, nrow = max(t_draws), ncol=dim(t_draws)[1])
for (t in 1:dim(t_draws)[1]) {
  for (i in 1:dim(t_draws)[2]) {
    r[t_draws[t,i],t] <- r[t_draws[t,i],t] + 1
  }
}

x <- paste0("t", seq(1,dim(t_draws)[1]))
y <- paste0("r", seq(1,max(t_draws)))
data <- expand.grid(X=x, Y=y)
data$density <- c(t(r))
 
# Heatmap 
ggplot(data, aes(X, Y, fill= density)) + 
  geom_tile() + theme(
    title = element_text(size=40),
    axis.text.x = element_blank(),
    axis.text.y = element_blank(),
    axis.title=element_text(size=30,face="bold"),
    axis.ticks = element_blank()
  ) + labs(title = "Posterior Draws of Run Length", 
           x = "Time", y = "Run Length")
```

#### MCMC Solution

```{r plot mcmc run length, fig.width=15,fig.height=5}
t_draws <- t_draws_mcmc
r = matrix(0, nrow = max(t_draws), ncol=dim(t_draws)[1])
for (t in 1:dim(t_draws)[1]) {
  for (i in 1:dim(t_draws)[2]) {
    r[t_draws[t,i],t] <- r[t_draws[t,i],t] + 1
  }
}

x <- paste0("t", seq(1,dim(t_draws)[1]))
y <- paste0("r", seq(1,max(t_draws)))
data <- expand.grid(X=x, Y=y)
data$density <- c(t(r))
 
# Heatmap 
ggplot(data, aes(X, Y, fill= density)) + 
  geom_tile() + theme(
    title = element_text(size=40),
    axis.text.x = element_blank(),
    axis.text.y = element_blank(),
    axis.title=element_text(size=30,face="bold"),
    axis.ticks = element_blank()
  ) + labs(title = "Posterior Draws of Run Length", 
           x = "Time", y = "Run Length")
```

### Convergence Diagnostics

```{r summary-analytic, warning=FALSE, message=FALSE}
fit_analytic_summary <- summary(fit_analytic, pars = c('meas'),
      digits_summary = 2, include = TRUE)
length(fit_analytic_summary$summary[fit_analytic_summary$summary[,"Rhat"]>1.05,"Rhat"])
```

For analytical solution there is no need for convergence diagnostics because we do not have any random parameters. 

```{r summary-mcmc, warning=FALSE, message=FALSE}
fit_mcmc_summary <- summary(fit_mcmc, pars = c('meas'),
      digits_summary = 2, include = TRUE)
min(fit_mcmc_summary$summary[,"n_eff"],na.rm = TRUE)
length(fit_mcmc_summary$summary[fit_mcmc_summary$summary[,"Rhat"]>1.05,"Rhat"])

```

As we can see that the minimum Rhat values along all the parameters and changepoint estimations are 1, that is our model fitted very well.  

The effective sample sizes are looking also good. The ration n_eff / n_transitions is greater than 0.001 for all parameters. 

Here we check the tree depth, E-BFMI and divergences. All are looking good. 

```{r Diagnostics-mcmc}
check_treedepth(fit_mcmc)
check_energy(fit_mcmc)
check_divergences(fit_mcmc)
```

# Bayesian Online Changepoint Detection (Syntetic Data)

In this section we have expanded our work to show how effectively Bayesian online changepoint detection algorithm is find more than one changepoints.

## Analytical Solution

```{r fit stan analytic on artificial data,echo=TRUE, results = 'hide', warning=FALSE, message=FALSE}
fit_syntetic_analytic <- stan("StanFiles/online_analytic.stan",
                              data=data_synthetic, 
                              algorithm = "Fixed_param",
                              iter=2000, warmup=200, chains=5)
```


```{r draws stan analytic on artificial data,echo=TRUE, results = 'hide', warning=FALSE, message=FALSE}
draws_syntetic_analytic <- extract(fit_syntetic_analytic, pars = c("meas"))
t_draws_syntetic_analytic <- t(draws_syntetic_analytic$meas)
```


```{r plot online artificial run length, fig.width=15,fig.height=5}
t_draws <- t_draws_syntetic_analytic
r = matrix(0, nrow = max(t_draws), ncol=dim(t_draws)[1])
for (t in 1:dim(t_draws)[1]) {
  for (i in 1:dim(t_draws)[2]) {
    r[t_draws[t,i],t] <- r[t_draws[t,i],t] + 1
  }
}

x <- paste0("t", seq(1,dim(t_draws)[1]))
y <- paste0("r", seq(1,max(t_draws)))
data <- expand.grid(X=x, Y=y)
data$density <- c(t(r))
 
# Heatmap 
ggplot(data, aes(X, Y, fill= density)) + 
  geom_tile() + theme(
    title = element_text(size=40),
    axis.text.x = element_blank(),
    axis.text.y = element_blank(),
    axis.title=element_text(size=30,face="bold"),
    axis.ticks = element_blank()
  ) + labs(title = "Posterior Draws of Run Length", 
           x = "Time", y = "Run Length")
```


## MCMC Solution

```{r fit stan on artificial data mcmc,echo=TRUE, results = 'hide', warning=FALSE, message=FALSE}
fit_syntetic_mcmc <- stan("StanFiles/online_mcmc.stan",
                          data=data_synthetic,
                          iter=2500, warmup=500, chains=5)
```


```{r draws stan on artificial data mcmc,echo=TRUE, results = 'hide', warning=FALSE, message=FALSE}
draws_syntetic_mcmc <- extract(fit_syntetic_mcmc, pars = c("meas"))
t_draws_syntetic_mcmc <- t(draws_syntetic_mcmc$meas)
```

```{r plot online artificial run length mcmc, fig.width=15,fig.height=5}
t_draws <- t_draws_syntetic_mcmc
r = matrix(0, nrow = max(t_draws), ncol=dim(t_draws)[1])
for (t in 1:dim(t_draws)[1]) {
  for (i in 1:dim(t_draws)[2]) {
    r[t_draws[t,i],t] <- r[t_draws[t,i],t] + 1
  }
}

x <- paste0("t", seq(1,dim(t_draws)[1]))
y <- paste0("r", seq(1,max(t_draws)))
data <- expand.grid(X=x, Y=y)
data$density <- c(t(r))
 
# Heatmap 
ggplot(data, aes(X, Y, fill= density)) + 
  geom_tile() + theme(
    title = element_text(size=40),
    axis.text.x = element_blank(),
    axis.text.y = element_blank(),
    axis.title=element_text(size=30,face="bold"),
    axis.ticks = element_blank()
  ) + labs(title = "Posterior Draws of Run Length", 
           x = "Time", y = "Run Length")
```

# Conclusion

The multiple changepoint model (MCM) and hierarchical multiple changepoint model(HMCM) are performing in a similar fashion. However, when we compare the standard deviations of the parameters for MCM and HMCM, we see that HMCM has slightly lower standard deviation then MCM. 
It can also be seen from the histograms of the estimate for second changepoint, $s_2$.

In the online changepoint model we have compared our MCMC draws results with analytical solution of the problem. 

Overall, Bayesian Online Changepoint Model is superior to other models. It can find any number of changepoints in linear time and it converges much faster. In this project we implemeted Poisson observation and Gamma underlying process version of the algorithm. 

We specifically used conjugate prior model in order to compare with the analytical solution. As a future work, more generic versions of the MCMC solution of the model can be implemented. Because it can be used with models which do not have conjugate priors. This allows more complex models. 



